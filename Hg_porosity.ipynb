{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porosimetry Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing custom functions\n",
    "import sys\n",
    "sys.path.append('/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/python_analysis/common_functions')\n",
    "from custom_functions import *\n",
    "from common_fits import *\n",
    "\n",
    "import os\n",
    "import io\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import iqr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from statannot import add_stat_annotation\n",
    "import itertools\n",
    "import shutil\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/python_analysis/porosimetry\n"
     ]
    }
   ],
   "source": [
    "#Initialisation of data read\n",
    "#Root location\n",
    "root = '/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/'\n",
    "#current location\n",
    "location = os.getcwd()\n",
    "print(location)\n",
    "\n",
    "#Data set locations\n",
    "#Hollow fibre micro-fibre analysis\n",
    "#Effect of pyridine concentration with S4 polymer solution on HF membrane porosity\n",
    "hf_s4_pyrid = root+'transport/porosimetery/raw_data/hollow_fibre/txt/'\n",
    "#Effect of pyridine concentration with S4 polymer solution on flatsheet membrane porosity\n",
    "fs_s4_pyrid = root+'transport/porosimetery/raw_data/flatsheet/txt/'\n",
    "#Roxanna's Data\n",
    "R_dat_loc = root+'transport/porosimetery/Roxanna/'\n",
    "\n",
    "#Penetrometer blank data\n",
    "blank_loc = root+'transport/porosimetery/blanks/txt/'\n",
    "\n",
    "#pressure table\n",
    "p_table = np.loadtxt(root+'transport/porosimetery/raw_data/pressure_table.txt', delimiter=\"\\t\")\n",
    "#/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/transport/porosimetery/raw_data\n",
    "\n",
    "#List of all data sets to be analysed\n",
    "data_sets = [hf_s4_pyrid,fs_s4_pyrid]\n",
    "\n",
    "#sample key location\n",
    "sample_key_name = 'sample_key_17032020'\n",
    "sample_key = '/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/sample_keys/'+sample_key_name+'.xlsx'\n",
    "\n",
    "#Import sample key\n",
    "sample_key = pd.read_excel(sample_key)\n",
    "\n",
    "#Set location of processed data\n",
    "processed = location+'/processed/'\n",
    "checkdir(processed) #check that processed data directory exists\n",
    "#location of processed collated porosimetry data\n",
    "collated_data= processed +'/'+'collated_data'+'/'\n",
    "checkdir(collated_data) #check that collated data directory exists\n",
    "#location of processed and binned raw distribtions\n",
    "raw_dists = processed+'raw_dist/'\n",
    "checkdir(raw_dists) #check that processed recreated raw distribtion data directory exists\n",
    "#location of the blanked data\n",
    "blank_sav = processed +'blanked/'\n",
    "checkdir(blank_sav) #check that blanked data save location exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables to be considered to separate out distributions\n",
    "variables = ['pyridine_conc','flat_or_fibre']\n",
    "#create a dictionary of variable titles for each of the variables considered\n",
    "variable_label = {'solution_name':'Polymer Solution','pyridine_conc':'Pyridine Concentration (PPM)','wire_speed':'Wire Speed (mm/s)',\n",
    "                  'rotation_speed':'Rotation Speed (degrees/s)','flow_rate':'Flow Rate (ml/hr)','flat_or_fibre':'Membrane Morphology'} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Penetrometer parameters\n",
    "Penetrometers = {'14-0783':{'Penetrometer':'3 Bulb, 06-11, Stem, Solid','stem_vol':0.4120, 'pen_vol':3.2121, 'pen_weight':62.4435},\n",
    "                '14-0638':{'Penetrometer':'3 Bulb, 06-11, Stem, Solid','stem_vol':0.4120, 'pen_vol':3.2121, 'pen_weight':62.6965}}\n",
    "\n",
    "#Hg parameters\n",
    "hg_contact_angle = 130 * (np.pi/180) #degrees\n",
    "hg_surface_tension = 485 #dynes/cm\n",
    "hg_density = 13.5335 #g/ml\n",
    "\n",
    "#porosity data processing constants\n",
    "#Washburn constant\n",
    "WASHCON = (10**4)/68947.6 #((um/cm)/(dynes/((cm**2)*psia)))\n",
    "\n",
    "#Accepted R^2 value\n",
    "accept_r2 = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common import function from TXT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TXT_import(data_loc,data,penetrometer):\n",
    "    ##Opening file    \n",
    "    #select file,open it and create handle for the file\n",
    "    fhand = codecs.open(os.path.join(data_loc,data),'r', 'iso-8859-1')\n",
    "    \n",
    "    #Extracting intrustion data\n",
    "    #to find data within txt file start counters to specify line within txt file\n",
    "    count = 0\n",
    "    #create place holders for where want to slice txt file\n",
    "    start = 0\n",
    "    end = 0\n",
    "    #create string of all data in file\n",
    "    ex_data = []\n",
    "    #for each line in the file\n",
    "    for line in fhand:\n",
    "        #add one to counter to keep track of which file line\n",
    "        count = count + 1\n",
    "        #remove (strip) white space\n",
    "        line = line.strip()\n",
    "        #add the each line to the string of all data\n",
    "        ex_data.append(line)\n",
    "        #find line in file which starts with tabular, as this is beginning of raw p-v data in file\n",
    "        if 'Cumulative Intrusion vs Pressure' in line:\n",
    "            start = count\n",
    "        #find line in file 'Cumulative Intrusion vs Pressure', as this is beginning of next section after raw p-v data in file\n",
    "        if 'Incremental Intrusion vs Pressure' in line:\n",
    "            end = count\n",
    "    #slice string with data between beginning and end of raw data to extract only data\n",
    "    ex_data = ex_data[start:end]\n",
    "    #print(data)\n",
    "    \n",
    "    #Having removed data bounds from bulk file, will now trim to extract only numerical data\n",
    "    #to find data within txt file start counter to specify line within txt file\n",
    "    count = 0\n",
    "    #create place holders for where want to slice txt file\n",
    "    start = 0\n",
    "    end = 0\n",
    "    #for each line withing the data\n",
    "    for line in ex_data:\n",
    "        #add one to counter to keep track of which file line\n",
    "        count = count + 1\n",
    "        #find line in file which starts with ----:, as this is beginning of raw p-v numerical data in file\n",
    "        if line.startswith('Pressure (psia)'):\n",
    "            start = count\n",
    "        #take note of point just before MICROMETRICS as this is where numerical data ends within trimmed data file    \n",
    "        if line.startswith('MICROMERITICS'):\n",
    "            end = count - 4\n",
    "    #slice string with data between beginning and end of raw data to extract only numerical data\n",
    "    ex_data = ex_data[start:end]\n",
    "    #print(ex_data)\n",
    "\n",
    "    #Having extracted string of all numerical data need to convert into pandas dataframe\n",
    "    pddata = pd.read_csv(io.StringIO('\\n'.join(ex_data)),header=None, delim_whitespace=True)\n",
    "    #With the correct column names\n",
    "    pddata.columns = ['pressure', 'cumulative_intrusion']    \n",
    "    #print(pddata.head())\n",
    "    \n",
    "    #Bin cumulative_intrusions to standard pressure table to allow for constant pressure table to be used and thereby allow for averaging data\n",
    "    bin_array = p_table\n",
    "    #bin data according to pressure table\n",
    "    bin_sum, bin_edges, binnumber = stats.binned_statistic(pddata['pressure'], pddata['cumulative_intrusion'], statistic ='sum', bins = bin_array)\n",
    "    #finding the middle of each bin \n",
    "    bin_width = (bin_edges[1] - bin_edges[0])\n",
    "    bin_centers = bin_edges[1:] - bin_width/2\n",
    "    #creating binned dataframe with sum of each bin as column\n",
    "    pddata = pd.DataFrame({'pressure':bin_centers, 'cumulative_intrusion':bin_sum})\n",
    "    #print(pddata)\n",
    "    #replace 0 values with previous value in table\n",
    "    #make list for values\n",
    "    P_dat = []\n",
    "    for i in range(len(pddata['cumulative_intrusion'])):\n",
    "        if i == 0:\n",
    "            P_dat.append(pddata['cumulative_intrusion'].iloc[i])\n",
    "        elif pddata['cumulative_intrusion'].iloc[i] == 0:\n",
    "            P_dat.append(P_dat[i - 1])\n",
    "        #elif P_dat[i-1] > pddata['cumulative_intrusion'].iloc[i]:\n",
    "        #    P_dat.append(P_dat[i - 1])\n",
    "        else:\n",
    "            P_dat.append(pddata['cumulative_intrusion'].iloc[i])\n",
    "            pass\n",
    "    #print(P_dat)\n",
    "    pddata['cumulative_intrusion'] = P_dat\n",
    "    \n",
    "    #Extract total volume of mercury injected into penetrometer\n",
    "    V_tot = P_dat[-1] \n",
    "    \n",
    "    ##Extracting sample weight\n",
    "    #select file,open it and create handle for the file\n",
    "    fhand = codecs.open(os.path.join(data_loc,data),'r', 'iso-8859-1')\n",
    "    #to find data within txt file start counters to specify line within txt file\n",
    "    count = 0\n",
    "    #create place holders for where want to slice txt file\n",
    "    start = 0\n",
    "    end = 0\n",
    "    #create string of all data in file\n",
    "    ex_data = []\n",
    "    #for each line in the file\n",
    "    for line in fhand:\n",
    "        #remove (strip) white space\n",
    "        line = line.strip()\n",
    "        #add the each line to the string of all data\n",
    "        ex_data.append(line)\n",
    "        #find line in file which starts with tabular, as this is beginning of raw p-v data in file\n",
    "        if 'Sample Weight' in line:\n",
    "            start = count\n",
    "            end = count + 1\n",
    "        #add one to counter to keep track of which file line\n",
    "        count = count + 1\n",
    "    #slice string with data between beginning and end of raw data to extract only data\n",
    "    ex_data = ex_data[start:end]\n",
    "    sample_weight = float(ex_data[0][-8:-2])\n",
    "    \n",
    "    ##Extracting assembly weight\n",
    "    #select file,open it and create handle for the file\n",
    "    fhand = codecs.open(os.path.join(data_loc,data),'r', 'iso-8859-1')\n",
    "    #to find data within txt file start counters to specify line within txt file\n",
    "    count = 0\n",
    "    #create place holders for where want to slice txt file\n",
    "    start = 0\n",
    "    end = 0\n",
    "    #create string of all data in file\n",
    "    ex_data = []\n",
    "    #for each line in the file\n",
    "    for line in fhand:\n",
    "        #remove (strip) white space\n",
    "        line = line.strip()\n",
    "        #add the each line to the string of all data\n",
    "        ex_data.append(line)\n",
    "        #find line in file which starts with tabular, as this is beginning of raw p-v data in file\n",
    "        if 'Assembly Weight:' in line:\n",
    "            start = count\n",
    "            end = count + 1\n",
    "        #add one to counter to keep track of which file line\n",
    "        count = count + 1\n",
    "    #slice string with data between beginning and end of raw data to extract only data\n",
    "    ex_data = ex_data[start:end]\n",
    "    #print(ex_data)\n",
    "    assembly_weight = float(ex_data[0][-13:-3])\n",
    "    #print(assembly_weight)\n",
    "    \n",
    "    ###Begin processing extracted data\n",
    "    ##Calculating pore size distribution\n",
    "    #convert pressures into characteristic pore diameters\n",
    "    #get conversion equation from micromeritics data processing\n",
    "    #D = (WASHCON*y*(-4cos(q)))/P\n",
    "    #Calculate diameters from associated pressure and then convert diameters from micrometers into nanometers\n",
    "    pddata['pore_diameter'] = ((WASHCON*hg_surface_tension*(-4*np.cos(hg_contact_angle)))/pddata['pressure'])* 10**3\n",
    "    \n",
    "    #calculate incremental specific intrusion volume\n",
    "    #from micromeritics data processing\n",
    "    #Iii = Ii - Ii-1 \n",
    "    Ii = []\n",
    "    counter = 0\n",
    "    for intrusion in pddata['cumulative_intrusion']:\n",
    "        if counter == 0:\n",
    "            Iii = 0\n",
    "        else:\n",
    "            Iii = pddata.iloc[counter]['cumulative_intrusion'] - pddata.iloc[counter-1]['cumulative_intrusion']\n",
    "        Ii.append(Iii)\n",
    "        counter = counter + 1\n",
    "\n",
    "    #calculate differential specific incremental intrusion volume\n",
    "    #from micromeritics data processing\n",
    "    #Idi = Ii/(Di - (Di-1))\n",
    "    dI = []\n",
    "    counter = 0\n",
    "    for intrusion in Ii:\n",
    "        if counter == 0:\n",
    "            dIi = (-Ii[counter])/(pddata['pore_diameter'].iloc[counter])\n",
    "        else:\n",
    "            dIi = (-Ii[counter])/(pddata['pore_diameter'].iloc[counter]-pddata['pore_diameter'].iloc[counter-1])\n",
    "        dI.append(dIi)\n",
    "        counter = counter + 1\n",
    "    \n",
    "    \n",
    "    #calculate log differential diameter incremental intrusion volume\n",
    "    #from micromeritics data processing\n",
    "    #Idi = Ii/(log(Di) - log((Di-1)))\n",
    "    log_dI = []\n",
    "    counter = 0\n",
    "    for intrusion in Ii:\n",
    "        if counter == 0:\n",
    "            log_dIi = (-Ii[counter])/(np.log10(pddata['pore_diameter'].iloc[counter]))\n",
    "        else:\n",
    "            log_dIi = (-Ii[counter])/(np.log10(pddata['pore_diameter'].iloc[counter])-np.log10(pddata['pore_diameter'].iloc[counter-1]))\n",
    "        log_dI.append(log_dIi)\n",
    "        counter = counter + 1\n",
    "    \n",
    "    #print(pddata)\n",
    "    pddata = pd.concat([pddata,pd.DataFrame({'SIV':np.transpose(Ii),'DSIV':np.transpose(dI),'log_DSIV':np.transpose(log_dI)})], axis=1)\n",
    "    #print(pddata)\n",
    "\n",
    "    #retruning dataframe\n",
    "    return pddata, assembly_weight,sample_weight,V_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import blanking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank_14-0783_06-11_15012020.TXT\n",
      "Blank_14-0638_06-11_04022020.txt\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(blank_loc):\n",
    "    if filename.endswith(\".TXT\") or filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        #identify penetrometer blank associated with\n",
    "        penetrometer = str(sample_key.loc[sample_key['porosimetery_filename'] == filename, 'penetrometer'].iloc[0])\n",
    "        #Open and process blank data file\n",
    "        blank_df, assembly_weight,sample_weight,b_V_tot = TXT_import(blank_loc,filename,penetrometer)\n",
    "        #Save processed pore distribution file to csv\n",
    "        blank_df.to_csv(blank_sav+penetrometer+'.csv', index=False)\n",
    "        #save blank pore information\n",
    "        b_pore_info = [assembly_weight,sample_weight,b_V_tot]\n",
    "        b_pore_info = pd.DataFrame((np.asarray(b_pore_info)))\n",
    "        #print(blank_df.head())\n",
    "        b_pore_info.to_csv(blank_sav+penetrometer+'.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & processing of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Having processed blanking data file, will use this baseline to correct error in all imported data\n",
    "#first import blank file so do not have to repeat for raw data file opened \n",
    "#blank_df = pd.read_csv(blank_sav+'blank_data.csv')\n",
    "#print(blank_df.head())\n",
    "for data_loc in data_sets:\n",
    "    for filename in os.listdir(data_loc):\n",
    "        if filename.endswith(\".TXT\") or filename.endswith(\".txt\"):\n",
    "            #print(filename)\n",
    "            #Intiially identify which penetrometer should be used\n",
    "            penetrometer = str(sample_key.loc[sample_key['porosimetery_filename'] == filename, 'penetrometer'].iloc[0])\n",
    "            #and open the associated penetrometer data file\n",
    "            blank_df = pd.read_csv(blank_sav+penetrometer+'.csv')\n",
    "            #import b_pore_info\n",
    "            b_pore_info = pd.read_csv(blank_sav+penetrometer+'.txt',sep=\"\\t\",header=None)\n",
    "            \n",
    "            #Open dataset and extract poresize distribution, Hg_volume and skeletal_volume\n",
    "            df,assembly_weight,sample_weight, V_tot = TXT_import(data_loc,filename,penetrometer)\n",
    "            #print(df)\n",
    "            #print(sample_weight)\n",
    "            \n",
    "            #to account for insturmental noise, want to subtract all blanked columns from sample data except for associated pore sizes\n",
    "            #use difference to find columns which differ\n",
    "            cols = df.columns.difference(['pressure','pore_diameter'])\n",
    "            #then subtract columns from blank file from raw data\n",
    "            df[cols] = df[cols].sub(blank_df[cols])\n",
    "            #re-sort the dataframe according to it index\n",
    "            df = df.reset_index(drop=True)\n",
    "            #print(df)\n",
    "            \n",
    "            ## Processing porosity data\n",
    "            ##Calculating bulk of enveloping volume\n",
    "            #Import penetrometer info\n",
    "            pen_weight = Penetrometers[penetrometer]['pen_weight']\n",
    "            #Initially calculate the mass of mercury pressent in pentomieter at atmospheric pressure = Assembly mass - (Pentometer mass + sample mass)\n",
    "            Hg_weight = assembly_weight - (pen_weight + sample_weight) # (g)\n",
    "            #calculate volume of Hg from density\n",
    "            Hg_volume = Hg_weight/hg_density # (ml)\n",
    "            #Extract skeletal volume from final value of cumulative intrusion volume\n",
    "            #V_tot = df['cumulative_intrusion'].iloc[-1] * sample_weight #(ml)\n",
    "            V_tot = df['cumulative_intrusion'].iloc[-1] \n",
    "            #calculate bulk volume\n",
    "            Vb = (Penetrometers[penetrometer]['pen_vol'] - Hg_volume)/sample_weight\n",
    "            #calcluate porosity\n",
    "            df.loc[0, 'porosity'] = (V_tot/Vb)*100\n",
    "            #print((V_tot/Vb)*100)\n",
    "            \n",
    "            \n",
    "            ##Adding in meta data associated with each curve\n",
    "            for variable in variables:\n",
    "                #save associated meta data to the first line of the dataframe\n",
    "                df.loc[0, variable] = sample_key.loc[sample_key['porosimetery_filename'] == filename, variable].iloc[0]\n",
    "            #add in unique identifier for easy additional metadata identification later on\n",
    "            df.loc[0, 'unique_id'] = int(sample_key.loc[sample_key['porosimetery_filename'] == filename, 'unique_id'].iloc[0])\n",
    "            #print(df.head())\n",
    "\n",
    "            #Saving out binned dataset as a csv file so may be recalled later\n",
    "            df.to_csv(raw_dists+str(filename[:-4])+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and processing of Roxanna's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data frame to hold all of roxanna's data in\n",
    "Rox_dat = pd.DataFrame()\n",
    "for filename in os.listdir(R_dat_loc):\n",
    "    if filename.endswith(\".TXT\") or filename.endswith(\".txt\"):\n",
    "        #print(filename)\n",
    "        #identify penetrometer blank associated with\n",
    "        penetrometer = str(sample_key.loc[sample_key['porosimetery_filename'] == filename, 'penetrometer'].iloc[0])\n",
    "        #Open and process blank data file\n",
    "        df = pd.read_csv(os.path.join(R_dat_loc,filename), sep=\"\\t\",header=None)\n",
    "        #print(df.head())\n",
    "        #extract pressure and cumulitive intrusion data\n",
    "        pddata = pd.DataFrame({'pressure':df[0].iloc[1:],'cumulative_intrusion':df[1].iloc[1:]})\n",
    "        #As headers are not taken, entire column initially strings. Convert to floats for binning\n",
    "        pddata = pddata.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        #Bin cumulative_intrusions to standard pressure table to allow for constant pressure table to be used and thereby allow for averaging data\n",
    "        bin_array = p_table\n",
    "        #bin data according to pressure table\n",
    "        bin_sum, bin_edges, binnumber = stats.binned_statistic(pddata['pressure'], pddata['cumulative_intrusion'], statistic ='sum', bins = bin_array)\n",
    "        #finding the middle of each bin \n",
    "        bin_width = (bin_edges[1] - bin_edges[0])\n",
    "        bin_centers = bin_edges[1:] - bin_width/2\n",
    "        #creating binned dataframe with sum of each bin as column\n",
    "        pddata = pd.DataFrame({'pressure':bin_centers, 'cumulative_intrusion':bin_sum})\n",
    "        #print(pddata)\n",
    "        #replace 0 values with previous value in table\n",
    "        #make list for values\n",
    "        P_dat = []\n",
    "        for i in range(len(pddata['cumulative_intrusion'])):\n",
    "            if i == 0:\n",
    "                P_dat.append(pddata['cumulative_intrusion'].iloc[i])\n",
    "            elif pddata['cumulative_intrusion'].iloc[i] == 0:\n",
    "                P_dat.append(P_dat[i - 1])\n",
    "            else:\n",
    "                P_dat.append(pddata['cumulative_intrusion'].iloc[i])\n",
    "                pass\n",
    "        #save the cumulative intrusion as that, with zeros replaced\n",
    "        pddata['cumulative_intrusion'] = P_dat\n",
    "        #print(pddata['cumulative_intrusion'])\n",
    "        \n",
    "        #Import sample weight\n",
    "        sample_weight = float(df[4].iloc[0])\n",
    "        #Import assembly weight\n",
    "        assembly_weight = float(df[4].iloc[1])\n",
    "        \n",
    "        #calculate porosity\n",
    "        #Import penetrometer info\n",
    "        pen_weight = Penetrometers[penetrometer]['pen_weight']\n",
    "        #Initially calculate the mass of mercury pressent in pentomieter at atmospheric pressure = Assembly mass - (Pentometer mass + sample mass)\n",
    "        Hg_weight = assembly_weight - (pen_weight + sample_weight) # (g)\n",
    "        #calculate volume of Hg from density\n",
    "        Hg_volume = Hg_weight/hg_density # (ml)\n",
    "        #Extract skeletal volume from final value of cumulative intrusion volume\n",
    "        #V_tot = df['cumulative_intrusion'].iloc[-1] * sample_weight #(ml)\n",
    "        V_tot = pddata['cumulative_intrusion'].iloc[-1] \n",
    "        #skeletal_volume = pddata['cumulative_intrusion'].iloc[-1]\n",
    "\n",
    "        #secondary method for porosimetry\n",
    "        Vb = (Penetrometers[penetrometer]['pen_vol'] - Hg_volume)/sample_weight\n",
    "        #calcluate porosity\n",
    "        #df.loc[0, 'porosity'] = (V_tot/Vb)*100\n",
    "        porosity = (V_tot/Vb)*100\n",
    "        \n",
    "        #add to Roxanna's dataframe\n",
    "        Rox_dat.loc[filename,'porosity'] = porosity\n",
    "\n",
    "#Sort Roxanna dataframe from highest to lowest\n",
    "#Rox_dat = Rox_dat.sort_values(columns='porosity',axis=1)\n",
    "Rox_dat.sort_values(by=['porosity'], inplace=True)\n",
    "#print(Rox_dat)\n",
    "#save roxanna's data frame out\n",
    "Rox_dat.to_csv(processed+'/rox_dat/'+'rox_dat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation of distributions by variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initially create a dictionary of all files which have the same parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'25.0,0.0': ['S4_25PPM_POLYU_2.csv', 'S4_25PPM_POLYU_3.csv', 'S4_25PPM_POLYU_1.csv'], '25.0,1.0': ['S4_25PPM_F_2.csv', 'S4_25PPM_F_3.csv', 'S4_25PPM_F_1.csv'], '5.0,1.0': ['S4_5PPM_F_3.csv', 'S4_5PPM_F_2.csv', 'S4_5PPM_F_1.csv'], '0.0,1.0': ['S4_0PPM_F_7.csv', 'S4_0PPM_F_9.csv', 'S4_0PPM_F_8.csv'], '50.0,0.0': ['S4_50PPM_POLYU_6.csv', 'S4_50PPM_POLYU_5.csv', 'S4_50PPM_POLYU_4.csv'], '10.0,1.0': ['S4_10PPM_F_3.csv', 'S4_10PPM_F_2.csv', 'S4_10PPM_F_1.csv'], '0.0,0.0': ['S4_0PPM_POLYU_3.csv', 'S4_0PPM_POLYU_2.csv', 'S4_0PPM_POLYU_1.csv'], '1.0,0.0': ['S4_1PPM_POLYU_1.csv', 'S4_1PPM_POLYU_2.csv', 'S4_1PPM_POLYU_3.csv'], '1.0,1.0': ['S4_1PPM_F_1.csv', 'S4_1PPM_F_3.csv', 'S4_1PPM_F_2.csv'], '10.0,0.0': ['S4_10PPM_POLYU_3.csv', 'S4_10PPM_POLYU_2.csv', 'S4_10PPM_POLYU_1.csv'], '50.0,1.0': ['S4_50PPM_F_2.csv', 'S4_50PPM_F_3.csv', 'S4_50PPM_F_1.csv'], '5.0,0.0': ['S4_5PPM_POLYU_3.csv', 'S4_5PPM_POLYU_2.csv', 'S4_5PPM_POLYU_1.csv']}\n"
     ]
    }
   ],
   "source": [
    "para_comb = SimilarSetCollation(raw_dists,variables,sample_key,'porosimetery_filename','txt')\n",
    "print(para_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of files with the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially itterating through each of the different filename associated with each of the parameter combination identifiers\n",
    "for identifier in para_comb:\n",
    "    #initilise counter to keep track of how many files are being considered for each parameter set\n",
    "    file_count = 0\n",
    "    #As want to initially keep extracted intrusion data separate from pressure table create holding dataframe\n",
    "    ex_df = pd.DataFrame()\n",
    "    #At the same time want to extract porosity data, create list of extracted porosity data\n",
    "    ex_poro = []\n",
    "    #make list of unique ideitifiers of averaged datasets\n",
    "    uni_ids = []\n",
    "    for filename in para_comb[identifier]:\n",
    "        #print(filename)\n",
    "        #Initially want to open each file but check that is csv file\n",
    "        if filename.endswith(\".csv\"):\n",
    "            #open data file\n",
    "            df = pd.read_csv(raw_dists+filename, index_col = 0)\n",
    "            #print(df.head())\n",
    "            #To begin averaging data between data sets extract pressure and cumulative intrusion for each\n",
    "            #as only want to extract pore diameter table once only extract if considering first file\n",
    "            if file_count == 0:\n",
    "                para_df = df['pore_diameter'].copy()          \n",
    "            #having extracted pressure table want to extract log differential diameter incremental intrusion volume associated with each\n",
    "            ex_df = pd.concat([ex_df, df['log_DSIV']], axis=1,sort=False,ignore_index=True)\n",
    "            # At the same time extract \n",
    "            ex_poro.append(df.loc[0,'porosity'])\n",
    "            #extract unique identifiers\n",
    "            uni_ids.append(df.loc[0,'unique_id'])\n",
    "            #add to file_count to progress\n",
    "            file_count = file_count + 1\n",
    "    #having extracted the data from all datasets with the same parameter values want to calculate the average of each reading and associate with pressure again\n",
    "    para_df = pd.concat([para_df, ex_df.mean(axis=1)], axis=1,sort=False,ignore_index=True)\n",
    "    #print(para_df.head())\n",
    "    #rename column headers\n",
    "    para_df = para_df.rename(columns={0: \"pore_diameter\", 1: \"avg_log_DSIV\"})\n",
    "    #print(para_df.head())\n",
    "    \n",
    "    #append unique identifiers to df for ease of metadata identification\n",
    "    para_df = pd.concat([para_df, pd.DataFrame({'unique_id':uni_ids})], axis=1,sort=False)\n",
    "    #print(para_df.head())\n",
    "    #re-sort the dataframe according to it index\n",
    "    para_df = para_df.sort_index(axis=0)\n",
    "    ##Add associated meta_data to DataFrame and at the same time generate save name to reflect parameters used\n",
    "    #make list holder for save name\n",
    "    save_name = []\n",
    "    for variable in variables:\n",
    "        #add each variable to data frame in the first row\n",
    "        variable_val = sample_key.loc[sample_key['unique_id'] == df.loc[0,'unique_id'], variable].iloc[0]\n",
    "        para_df.loc[0, variable] = variable_val\n",
    "        #append to save name\n",
    "        save_name.append(variable+'-'+str(variable_val))\n",
    "        #print(df.head())\n",
    "    #strip save name\n",
    "    save_name = str(save_name).strip('[]').replace(\"'\",'').replace(' ','') \n",
    "    \n",
    "    #append porosity data to log differential intrusion volume - pressure DataFrame\n",
    "    para_df = pd.concat([para_df,pd.DataFrame(ex_poro,columns=['porosity'])], axis=1)\n",
    "    #print(para_df.head())\n",
    "    \n",
    "    ##Saving averaged data set para_df out\n",
    "    para_df.to_csv(collated_data+save_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation of distributions by variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pyridine_conc, 5.0': {'flat_or_fibre': ['pyridine_conc-5.0,flat_or_fibre-1.0.csv', 'pyridine_conc-5.0,flat_or_fibre-0.0.csv']}, 'flat_or_fibre, 1.0': {'pyridine_conc': ['pyridine_conc-5.0,flat_or_fibre-1.0.csv', 'pyridine_conc-1.0,flat_or_fibre-1.0.csv', 'pyridine_conc-0.0,flat_or_fibre-1.0.csv', 'pyridine_conc-25.0,flat_or_fibre-1.0.csv', 'pyridine_conc-10.0,flat_or_fibre-1.0.csv', 'pyridine_conc-50.0,flat_or_fibre-1.0.csv']}, 'pyridine_conc, 1.0': {'flat_or_fibre': ['pyridine_conc-1.0,flat_or_fibre-1.0.csv', 'pyridine_conc-1.0,flat_or_fibre-0.0.csv']}, 'flat_or_fibre, 0.0': {'pyridine_conc': ['pyridine_conc-5.0,flat_or_fibre-0.0.csv', 'pyridine_conc-1.0,flat_or_fibre-0.0.csv', 'pyridine_conc-25.0,flat_or_fibre-0.0.csv', 'pyridine_conc-10.0,flat_or_fibre-0.0.csv', 'pyridine_conc-50.0,flat_or_fibre-0.0.csv', 'pyridine_conc-0.0,flat_or_fibre-0.0.csv']}, 'pyridine_conc, 0.0': {'flat_or_fibre': ['pyridine_conc-0.0,flat_or_fibre-1.0.csv', 'pyridine_conc-0.0,flat_or_fibre-0.0.csv']}, 'pyridine_conc, 25.0': {'flat_or_fibre': ['pyridine_conc-25.0,flat_or_fibre-0.0.csv', 'pyridine_conc-25.0,flat_or_fibre-1.0.csv']}, 'pyridine_conc, 10.0': {'flat_or_fibre': ['pyridine_conc-10.0,flat_or_fibre-0.0.csv', 'pyridine_conc-10.0,flat_or_fibre-1.0.csv']}, 'pyridine_conc, 50.0': {'flat_or_fibre': ['pyridine_conc-50.0,flat_or_fibre-0.0.csv', 'pyridine_conc-50.0,flat_or_fibre-1.0.csv']}}\n"
     ]
    }
   ],
   "source": [
    "matched_para_comb = VariableSep(collated_data,variables)\n",
    "print(matched_para_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and deleting outputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pyridine_conc, 5.0': {'flat_or_fibre': ['pyridine_conc-5.0,flat_or_fibre-1.0.csv', 'pyridine_conc-5.0,flat_or_fibre-0.0.csv']}, 'flat_or_fibre, 1.0': {'pyridine_conc': ['pyridine_conc-5.0,flat_or_fibre-1.0.csv', 'pyridine_conc-1.0,flat_or_fibre-1.0.csv', 'pyridine_conc-0.0,flat_or_fibre-1.0.csv', 'pyridine_conc-25.0,flat_or_fibre-1.0.csv', 'pyridine_conc-10.0,flat_or_fibre-1.0.csv', 'pyridine_conc-50.0,flat_or_fibre-1.0.csv']}, 'pyridine_conc, 1.0': {'flat_or_fibre': ['pyridine_conc-1.0,flat_or_fibre-1.0.csv', 'pyridine_conc-1.0,flat_or_fibre-0.0.csv']}, 'flat_or_fibre, 0.0': {'pyridine_conc': ['pyridine_conc-5.0,flat_or_fibre-0.0.csv', 'pyridine_conc-1.0,flat_or_fibre-0.0.csv', 'pyridine_conc-25.0,flat_or_fibre-0.0.csv', 'pyridine_conc-10.0,flat_or_fibre-0.0.csv', 'pyridine_conc-50.0,flat_or_fibre-0.0.csv', 'pyridine_conc-0.0,flat_or_fibre-0.0.csv']}, 'pyridine_conc, 0.0': {'flat_or_fibre': ['pyridine_conc-0.0,flat_or_fibre-1.0.csv', 'pyridine_conc-0.0,flat_or_fibre-0.0.csv']}, 'pyridine_conc, 25.0': {'flat_or_fibre': ['pyridine_conc-25.0,flat_or_fibre-0.0.csv', 'pyridine_conc-25.0,flat_or_fibre-1.0.csv']}, 'pyridine_conc, 10.0': {'flat_or_fibre': ['pyridine_conc-10.0,flat_or_fibre-0.0.csv', 'pyridine_conc-10.0,flat_or_fibre-1.0.csv']}, 'pyridine_conc, 50.0': {'flat_or_fibre': ['pyridine_conc-50.0,flat_or_fibre-0.0.csv', 'pyridine_conc-50.0,flat_or_fibre-1.0.csv']}}\n"
     ]
    }
   ],
   "source": [
    "creation_and_destruction(processed,matched_para_comb)\n",
    "print(matched_para_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting pore size distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As Violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','') \n",
    "        #Make dataframe as holder for concatinated recreated data sets\n",
    "        con_df = pd.DataFrame()\n",
    "        #make dataframe for summary data\n",
    "        sum_df = pd.DataFrame()\n",
    "        #make list of not_matching variables for ordering\n",
    "        nm_order = []\n",
    "        #in each of these not matching parameters parase all of the listed files\n",
    "        for filename in matched_para_comb[matching][not_matching]:\n",
    "            #print(matching+not_matching)\n",
    "            #print(filename)\n",
    "            #create file to identify dataset\n",
    "            file = filename[:-4]\n",
    "            #print(file)\n",
    "            #open distribution data as dataframe\n",
    "            df = pd.read_csv(collated_data+filename, index_col = 0)\n",
    "            #print(df.head())\n",
    "            \n",
    "            #extract not_matching parameter value\n",
    "            #split between parameters\n",
    "            nm = file.split(',')\n",
    "            #split between parameter label and value\n",
    "            nm = [i.split('-') for i in nm]\n",
    "            #flatten list\n",
    "            #nm = [item for sublist in nm for item in sublist]\n",
    "            #print(nm)\n",
    "            #exctract parameter value based on location within variables list\n",
    "            #print(variables.index(not_matching))\n",
    "            nm = nm[variables.index(not_matching)][1]\n",
    "            #print(nm)\n",
    "            \n",
    "            #extend nm_order with not_matching parameter value\n",
    "            nm_order.append(nm)\n",
    "            \n",
    "            #change data label to variable value\n",
    "            df = df.rename(columns={'avg_log_DSIV':nm})\n",
    "            \n",
    "            #to allow for comparison of distributions want to look at normalised % frequency of each pore diameter\n",
    "            #create a list of relative frequecies normalised to the total intrustion volume\n",
    "            n_log_DSIV = ((df[nm].abs()/df[nm].abs().sum())*100).tolist()\n",
    "\n",
    "            #create for loop that looks for any value less than 2 and removes, this also removes any nans\n",
    "            for n, i in enumerate(n_log_DSIV):\n",
    "                if i < 1.5:\n",
    "                    n_log_DSIV[n] = float(0)\n",
    "                elif str(i) == 'nan':\n",
    "                    n_log_DSIV[n] = float(0)\n",
    "            #print(n_log_DSIV)\n",
    "\n",
    "            #create list of pore diamters\n",
    "            pore_d = df['pore_diameter'].tolist()\n",
    "\n",
    "            #create dictionary from list of frequencies and pore diameters\n",
    "            d = {'n_log_DSIV':n_log_DSIV,'pore_diameter':pore_d}\n",
    "            n_log_DSIV = pd.DataFrame(d)\n",
    "\n",
    "            #create a list comprising a given radius the length of the total frequencies for each row\n",
    "            rdf = sum([[row['pore_diameter']] * int(round(row['n_log_DSIV'])) for index, row in n_log_DSIV.iterrows()], [])\n",
    "            #convert this list into a pandas dataframe\n",
    "            rdf = pd.DataFrame({nm:rdf})\n",
    "                    \n",
    "            #Concatinate each recreated data set to eachother           \n",
    "            con_df = pd.concat([con_df, rdf[nm]], axis=1, sort=False)\n",
    "            #print(con_df)\n",
    "            \n",
    "            ##Calculating summary statistics from distriutions\n",
    "            median = np.percentile(rdf[nm].dropna(), 50)\n",
    "            IQR = iqr(rdf[nm].dropna())\n",
    "            #in addition to the IQR calculate 25 and 75th quartiles\n",
    "            quartile_25 = np.percentile(rdf[nm].dropna(), 25)\n",
    "            quartile_75 = np.percentile(rdf[nm].dropna(), 75)\n",
    "            mean = rdf[nm].mean()\n",
    "            SD = rdf[nm].std()\n",
    "            skew = rdf[nm].skew()\n",
    "            #from calculation of skew determine whether to use median or mean\n",
    "            if abs(skew) > 0.5:\n",
    "                stat = 'median'\n",
    "            else:\n",
    "                stat = 'mean'\n",
    "\n",
    "            #add each of the summary statstics and pair this with datasets\n",
    "            sum_df.loc[nm,'median'] = median\n",
    "            sum_df.loc[nm,'IQR'] = IQR\n",
    "            sum_df.loc[nm,'25_quartile'] = quartile_25\n",
    "            sum_df.loc[nm,'75_quartile'] = quartile_75\n",
    "            sum_df.loc[nm,'mean'] = mean\n",
    "            sum_df.loc[nm,'SD'] = SD\n",
    "            sum_df.loc[nm,'skew'] = skew\n",
    "            sum_df.loc[nm,'stat'] = stat\n",
    "            \n",
    "\n",
    "        #Check summary dataframe\n",
    "        #print(sum_df.head())\n",
    "        #save summary dataframe out as CSV\n",
    "        sum_df.to_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_pore_dist_summary'+'.csv')\n",
    "        \n",
    "        #convert multiple columns of data into single column with header as variable\n",
    "        data = pd.melt(con_df)\n",
    "\n",
    "        #Find order of plots according to max value\n",
    "        #ordered = data.groupby(['variable'])['value'].aggregate(np.median).reset_index().sort_values('value')\n",
    "        #ordered_var = ordered['variable'].tolist()\n",
    "        \n",
    "        #Find order of plots from magnitude of nm variable, determined by ordering nm_order\n",
    "        nm_order.sort(key = float, reverse=True)\n",
    "        #print(nm_order)\n",
    "\n",
    "        #create new template for figure\n",
    "        fig, ax = plt.subplots()\n",
    "        #plot violin plot into figure\n",
    "        v_plt = sns.violinplot( x='variable', y= 'value', cut=0, data = data, order = nm_order, ax = ax) #order = ordered_var,\n",
    "        \n",
    "        #insert statistical annotations\n",
    "        #before can add statistical annotation must create boxPairList from previous statistical comparison table\n",
    "        #set which variable list controls order\n",
    "        var_order = nm_order.copy()\n",
    "        #create list for boxpairlist\n",
    "        pre_boxPairList = []\n",
    "        #for count of number of o values\n",
    "        for index in range(len(var_order)):\n",
    "            #to ensure that all combinations are considered again copy the uniquevalues\n",
    "            avalues = var_order.copy() #colour hue\n",
    "            #removing fixed variable so only consider changing variables\n",
    "            avalues.remove(var_order[index])\n",
    "            #considereing the appending value\n",
    "            for index in range(len(avalues)):\n",
    "                #let a = the file name and the ovalue which corresponds to the number within the list and pair them\n",
    "                a = (avalues[index],var_order[index])\n",
    "                #add the pair to the list of boxed pairs\n",
    "                if avalues[index] != var_order[index]:\n",
    "                    if a not in pre_boxPairList:\n",
    "                        pre_boxPairList.append(a)\n",
    "                else: pass\n",
    "        #adding statistical annotation\n",
    "        #if len(pre_boxPairList) > 1:\n",
    "            #add_stat_annotation(x='variable', y= 'value', data = data, boxPairList=pre_boxPairList, \n",
    "            #                    test='Mann-Whitney', textFormat='star', loc='inside', verbose = 0,order = nm_order, ax = ax) #order = ordered_var,\n",
    "\n",
    "        #Set plot labels\n",
    "        #First add label to the x-axis to describe the variable considered\n",
    "        #Retreve xlabel axis associated with variable parameter\n",
    "        xlabel = variable_label[not_matching]\n",
    "\n",
    "        #Add correct x tick labels\n",
    "        #initially retreve the existing key labels\n",
    "        labels = [t.get_text()  for t in ax.get_xticklabels()]\n",
    "\n",
    "        xlabel_list = []\n",
    "        for label in labels:\n",
    "            #print(label)\n",
    "            #For polymer solution crossreference variable with name\n",
    "            if not_matching == 'solution_name':\n",
    "                #initilise dictionary of polymer solution names\n",
    "                polysolkey = {'0.0':'Trial','1.0':'Initial' ,'2.0':'S1','3.0':'S2','4.0':'S3','5.0':'S4','6.0':'S5'}\n",
    "                #change label to update polymer solution name\n",
    "                label = polysolkey[label]\n",
    "\n",
    "            #append list to list of variable labels\n",
    "            xlabel_list.append(label)\n",
    "        #using list of variable labels update x-axis tick labels\n",
    "        ax.set_xticklabels(xlabel_list)\n",
    "        #set the x and y axis labels\n",
    "        v_plt.set(xlabel=xlabel, ylabel='Pore Diameter (nm)') #\n",
    "        #set the xlabels rotation\n",
    "        v_plt.set_xticklabels(v_plt.get_xticklabels(),rotation = -0)\n",
    "        #plt.show()\n",
    "\n",
    "        #save plot out\n",
    "        fig.savefig(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_pore_dist.png',bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        #Close figure to hide previews\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting porosity as function of pyridine as box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\\\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','') \n",
    "        #Make dataframe as holder for concatinated data\n",
    "        con_df = pd.DataFrame()\n",
    "        #make list of not_matching variables for ordering\n",
    "        nm_order = []\n",
    "        #make dataframe for summary data\n",
    "        sum_df = pd.DataFrame()\n",
    "        #make list for all the unique file identifiers\n",
    "        uni_ids = []\n",
    "        #file counter\n",
    "        file_count = 0 \n",
    "        #in each of these not matching parameters parase all of the listed files\n",
    "        for filename in matched_para_comb[matching][not_matching]:\n",
    "            #print(matching+not_matching)\n",
    "            #print(filename)\n",
    "            #create file to identify dataset\n",
    "            file = filename[:-4]\n",
    "            #print(file)\n",
    "            #open distribution data as dataframe\n",
    "            df = pd.read_csv(collated_data+filename, index_col = 0)\n",
    "            #print(df.head())\n",
    "            \n",
    "            #extract not_matching parameter value\n",
    "            #split between parameters\n",
    "            nm = file.split(',')\n",
    "            #split between parameter label and value\n",
    "            nm = [i.split('-') for i in nm]\n",
    "            #flatten list\n",
    "            #nm = [item for sublist in nm for item in sublist]\n",
    "            #print(nm)\n",
    "            #exctract parameter value based on location within variables list\n",
    "            #print(variables.index(not_matching))\n",
    "            nm = nm[variables.index(not_matching)][1]\n",
    "            #print(nm)\n",
    "            \n",
    "            #extend nm_order with not_matching parameter value\n",
    "            nm_order.append(nm)\n",
    "            \n",
    "            #change data label to variable value\n",
    "            df = df.rename(columns={'porosity':nm})\n",
    "                                \n",
    "            #Concatinate each recreated data set to eachother           \n",
    "            con_df = pd.concat([con_df, df[nm]], axis=1, sort=False)\n",
    "            #print(con_df)\n",
    "            \n",
    "            ##Calculating summary statistics from distriutions\n",
    "            ##Calculating summary statistics from distriutions\n",
    "            median = np.percentile(df[nm].dropna(), 50)\n",
    "            IQR = iqr(df[nm].dropna())\n",
    "            #in addition to the IQR calculate 25 and 75th quartiles\n",
    "            quartile_25 = np.percentile(df[nm].dropna(), 25)\n",
    "            quartile_75 = np.percentile(df[nm].dropna(), 75)\n",
    "            mean = df[nm].mean()\n",
    "            SD = df[nm].std()\n",
    "            skew = df[nm].skew()\n",
    "            #from calculation of skew determine whether to use median or mean\n",
    "            if abs(skew) > 0.5:\n",
    "                stat = 'median'\n",
    "            else:\n",
    "                stat = 'mean'\n",
    "\n",
    "            #add each of the summary statstics and pair this with datasets\n",
    "            sum_df.loc[file_count,'not_matching'] = nm\n",
    "            sum_df.loc[file_count,'median'] = median\n",
    "            sum_df.loc[file_count,'IQR'] = IQR\n",
    "            sum_df.loc[file_count,'25_quartile'] = quartile_25\n",
    "            sum_df.loc[file_count,'75_quartile'] = quartile_75\n",
    "            sum_df.loc[file_count,'mean'] = mean\n",
    "            sum_df.loc[file_count,'SD'] = SD\n",
    "            sum_df.loc[file_count,'skew'] = skew\n",
    "            sum_df.loc[file_count,'stat'] = stat\n",
    "            #add column for not matching parameter\n",
    "            sum_df.loc[file_count,not_matching] = nm\n",
    "            #append unique IDs to list\n",
    "            uni_ids.extend(df['unique_id'].dropna().tolist())\n",
    "            \n",
    "            #progress file count\n",
    "            file_count = file_count +1 \n",
    "        \n",
    "        uni_ids = pd.DataFrame({'unique_id':uni_ids})\n",
    "        #print(uni_ids)\n",
    "        #print(con_df.head())\n",
    "        sum_df = pd.concat([sum_df,uni_ids], axis=1)\n",
    "        #Check summary dataframe\n",
    "        #print(sum_df.head())\n",
    "        #organise sum_df in terms of variable magniture\n",
    "        sum_df.index = sum_df.index.astype(float)\n",
    "        sum_df = sum_df.sort_index()\n",
    "        #save summary dataframe out as CSV\n",
    "        sum_df.to_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity_summary'+'.csv')\n",
    "        \n",
    "        #convert multiple columns of data into two columns with header as variable and associated data\n",
    "        data = pd.melt(con_df)\n",
    "        #having converted dataframe want to remove any NANs\n",
    "        data = data.dropna()\n",
    "\n",
    "        #Find order of plots according to max value\n",
    "        #ordered = data.groupby(['variable'])['value'].aggregate(np.median).reset_index().sort_values('value')\n",
    "        #ordered_var = ordered['variable'].tolist()\n",
    "        \n",
    "        #Find order of plots from magnitude of nm variable, determined by ordering nm_order\n",
    "        nm_order.sort(key = float, reverse=True)\n",
    "        #print(nm_order)\n",
    "\n",
    "        #create new template for figure\n",
    "        fig, ax = plt.subplots()\n",
    "        #plot box plot into figure\n",
    "        b_plt = sns.boxplot( x='variable', y= 'value', data = data, order = nm_order, ax = ax) #order = ordered_var,\n",
    "        \n",
    "        #insert statistical annotations\n",
    "        #before can add statistical annotation must create boxPairList from previous statistical comparison table\n",
    "        #set which variable list controls order\n",
    "        var_order = nm_order.copy()\n",
    "        #create list for boxpairlist\n",
    "        pre_boxPairList = []\n",
    "        #for count of number of o values\n",
    "        for index in range(len(var_order)):\n",
    "            #to ensure that all combinations are considered again copy the uniquevalues\n",
    "            avalues = var_order.copy() #colour hue\n",
    "            #removing fixed variable so only consider changing variables\n",
    "            avalues.remove(var_order[index])\n",
    "            #considereing the appending value\n",
    "            for index in range(len(avalues)):\n",
    "                #let a = the file name and the ovalue which corresponds to the number within the list and pair them\n",
    "                a = (avalues[index],var_order[index])\n",
    "                #add the pair to the list of boxed pairs\n",
    "                if avalues[index] != var_order[index]:\n",
    "                    if a not in pre_boxPairList:\n",
    "                        pre_boxPairList.append(a)\n",
    "                else: pass\n",
    "        #adding statistical annotation\n",
    "        if len(pre_boxPairList) > 1:\n",
    "            add_stat_annotation(x='variable', y= 'value', data = data, boxPairList=pre_boxPairList, \n",
    "                                test='Mann-Whitney', textFormat='star', loc='inside', verbose = 0,order = nm_order, ax = ax) #order = ordered_var,\n",
    "        \n",
    "        \n",
    "        #Set plot labels\n",
    "        #First add label to the x-axis to describe the variable considered\n",
    "        #Retreve xlabel axis associated with variable parameter\n",
    "        xlabel = variable_label[not_matching]\n",
    "\n",
    "        #Add correct x tick labels\n",
    "        #initially retreve the existing key labels\n",
    "        labels = [t.get_text()  for t in ax.get_xticklabels()]\n",
    "        #print(labels)\n",
    "\n",
    "        xlabel_list = []\n",
    "        for label in labels:\n",
    "            #print(label)\n",
    "            #For polymer solution crossreference variable with name\n",
    "            if 'solution_name' in not_matching:\n",
    "                #initilise dictionary of polymer solution names\n",
    "                polysolkey = {'0.0':'Trial','1.0':'Initial' ,'2.0':'S1','3.0':'S2','4.0':'S3','5.0':'S4','6.0':'S5'}\n",
    "                #change label to update polymer solution name\n",
    "                label = polysolkey[label]\n",
    "\n",
    "            #append list to list of variable labels\n",
    "            xlabel_list.append(label)\n",
    "        \n",
    "        \n",
    "        #print(xlabel_list)\n",
    "        #using list of variable labels update x-axis tick labels\n",
    "        b_plt.set_xticklabels(xlabel_list)\n",
    "        #set the x and y axis labels\n",
    "        b_plt.set(xlabel=xlabel, ylabel='Porosity (%)') #\n",
    "        #set the xlabels rotation\n",
    "        #b_plt.set_xticklabels(v_plt.get_xticklabels(),rotation = -0)\n",
    "        #plt.show()\n",
    "\n",
    "        #save plot out\n",
    "        fig.savefig(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity.png',bbox_inches='tight', dpi=300)\n",
    "                \n",
    "        #Close figure to hide previews\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porosity as a function of pyridine concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expfit(xdata,ydata,yerr,bounds):\n",
    "    from common_fits import LogFunc\n",
    "    import numpy as np\n",
    "    ####Fit log curve with all scalars#####\n",
    "    #using Scipy.optimise.curvefit then fit log function to the data to extract the curve parameters\n",
    "    #setting bounds to prevent -ve log values\n",
    "    bounds = bounds\n",
    "    #fit the curve\n",
    "    popt, pcov = curve_fit(ExpFunc,xdata, ydata,sigma=yerr,bounds=bounds)\n",
    "\n",
    "    #each of the coefficients for the LogFunc to then be plotted\n",
    "    a = popt[0]\n",
    "    b = popt[1]\n",
    "    c = popt[2]\n",
    "\n",
    "    #calculate R^2 to get the goodness of fit\n",
    "    residuals = ydata- ExpFunc(xdata, a,b,c)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    #print('log_fit'+str(r_squared))\n",
    "\n",
    "    #having found the parameters then plot fitted curve onto figure\n",
    "    min_x_val = xdata.min()\n",
    "    max_x_val = xdata.max()\n",
    "    expcurvex = np.linspace(min_x_val,max_x_val,1000)\n",
    "    expcurvey = ExpFunc(expcurvex,a,b,c)\n",
    "    \n",
    "    return (m,b,c,r_squared,logcurvex,logcurvey)\n",
    "\n",
    "def logfit(xdata,ydata,yerr,bounds):\n",
    "    from common_fits import LogFunc\n",
    "    import numpy as np\n",
    "    ####Fit log curve with all scalars#####\n",
    "    #using Scipy.optimise.curvefit then fit log function to the data to extract the curve parameters\n",
    "    #setting bounds to prevent -ve log values\n",
    "    bounds = bounds\n",
    "    #fit the curve\n",
    "    popt, pcov = curve_fit(LogFunc,xdata, ydata,sigma=yerr,bounds=bounds)\n",
    "\n",
    "    #each of the coefficients for the LogFunc to then be plotted\n",
    "    m = popt[0]\n",
    "    b = popt[1]\n",
    "    c = popt[2]\n",
    "\n",
    "    #calculate R^2 to get the goodness of fit\n",
    "    residuals = ydata- LogFunc(xdata, m,b,c)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    #print('log_fit'+str(r_squared))\n",
    "\n",
    "    #having found the parameters then plot fitted curve onto figure\n",
    "    min_x_val = xdata.min()\n",
    "    max_x_val = xdata.max()\n",
    "    logcurvex = np.linspace(min_x_val,max_x_val,1000)\n",
    "    logcurvey = LogFunc(logcurvex,m,b,c)\n",
    "    \n",
    "    return (m,b,c,r_squared,logcurvex,logcurvey)\n",
    "\n",
    "\n",
    "def linfit(xdata,ydata,yerr,bounds):\n",
    "    from common_fits import linFunc\n",
    "    import numpy as np\n",
    "    ##Fit linier function\n",
    "    #using Scipy.optimise.curvefit then fit lin function to the data to extract the curve parameters              \n",
    "    popt, pcov = curve_fit(linFunc,xdata, ydata, sigma = yerr)\n",
    "\n",
    "    #each of the coefficients for the LogFunc to then be plotted\n",
    "    slope = popt[0]\n",
    "    intercept = popt[1]\n",
    "\n",
    "    #calculate R^2 to get the goodness of fit\n",
    "    residuals = ydata- linFunc(xdata,slope,intercept)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    #print('lin_fit'+str(r_squared))\n",
    "\n",
    "    #having found the parameters then plot fitted curve onto figure\n",
    "    min_x_val = xdata.min()\n",
    "    max_x_val = xdata.max()\n",
    "    lincurvex = np.linspace(min_x_val,max_x_val,1000)\n",
    "    lincurvey = linFunc(lincurvex,slope,intercept)\n",
    "    \n",
    "    return (slope,intercept,r_squared,lincurvex,lincurvey)\n",
    "\n",
    "def tanhfit(xdata,ydata,yerr,bounds):\n",
    "    popt, pcov = curve_fit(tanhFunc,xdata, ydata,sigma=yerr)\n",
    "\n",
    "    #each of the coefficients for the LogFunc to then be plotted\n",
    "    a = popt[0]\n",
    "    b = popt[1]\n",
    "    c = popt[2]\n",
    "    d = popt[3]\n",
    "\n",
    "    #calculate R^2 to get the goodness of fit\n",
    "    residuals = ydata - tanhFunc(xdata, a,b,c,d)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    #print('tanh_fit'+str(r_squared))\n",
    "\n",
    "    #having found the parameters then plot fitted curve onto figure\n",
    "    min_x_val = xdata.min()\n",
    "    max_x_val = xdata.max()\n",
    "    tanhcurvex = np.linspace(min_x_val,max_x_val,1000)\n",
    "    tanhcurvey = tanhFunc(tanhcurvex,a,b,c,d) \n",
    "    \n",
    "    return (a,b,c,d,r_squared,tanhcurvex,tanhcurvey)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','') \n",
    "        #Make dataframe as holder for concatinated data\n",
    "        con_df = pd.DataFrame()\n",
    "        #make list of not_matching variables for ordering\n",
    "        nm_order = []\n",
    "        #make dataframe for summary data\n",
    "        sum_df = pd.DataFrame()\n",
    "        #in each of these not matching parameters parase all of the listed files\n",
    "        for filename in matched_para_comb[matching][not_matching]:\n",
    "            #print(matching+not_matching)\n",
    "            #print(filename)\n",
    "            #create file to identify dataset\n",
    "            file = filename[:-4]\n",
    "            #print(file)\n",
    "            #open distribution data as dataframe\n",
    "            df = pd.read_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity_summary'+'.csv', index_col = 0)\n",
    "            #print(df)\n",
    "            \n",
    "            \n",
    "            #Using matpltlib to plot instead of seaborn to plot summary data\n",
    "            #print(not_matching)\n",
    "            if 'solution_name' in not_matching or 'flat_or_fibre' in not_matching :\n",
    "                pass\n",
    "            else:\n",
    "                ###Plotting contineous summary data and fit curve if there are more than 3 data points\n",
    "                if len(df[not_matchings]) > 3:\n",
    "                    #Initilise subplots \n",
    "                    fig, ax = plt.subplots()\n",
    "                    #print('pop')\n",
    "\n",
    "                    #Using summary data table to extract data\n",
    "                    x = df[not_matching].dropna()\n",
    "                    xdata = np.asarray([1.0e-1 if x==0 else x for x in x])\n",
    "                    ydata = np.asarray([1.0e-1 if x==0 else x for x in df['median'].dropna()])\n",
    "                    yerr = ((df['median']-df['25_quartile']).dropna(),(df['75_quartile']-df['median']).dropna())\n",
    "                   \n",
    "                    ax.errorbar(xdata, ydata, yerr=yerr, fmt='o')\n",
    "                    ax.set(xlabel=xlabel, ylabel='Porosity (%)') #\n",
    "                    \n",
    "                    #Set plot labels\n",
    "                    #First add label to the x-axis to describe the variable considered\n",
    "                    #Retreve xlabel axis associated with variable parameter\n",
    "                    xlabel = variable_label[not_matching]\n",
    "\n",
    "                    #set the x and y axis labels\n",
    "                    ax.set(xlabel=xlabel, ylabel='Porosity (%)') #\n",
    "                    #set the xlabels rotation\n",
    "                    #ax.set_xticklabels(ax.get_xticklabels(),rotation = -0)\n",
    "                    #plt.show()\n",
    "\n",
    "                            ####Fit log curve with all scalars#####\n",
    "                    yerr = np.asarray([1.0e-1 if x==0 else x for x in df['SD'].dropna()])\n",
    "                    #using Scipy.optimise.curvefit then fit log function to the data to extract the curve parameters\n",
    "                    #setting bounds to prevent -ve log values\n",
    "                    bounds = (0,-np.inf, -np.inf), (np.inf, np.inf, np.inf)\n",
    "                    #fit the curve\n",
    "                    (m,b,c,r_squared,logcurvex,logcurvey) = logfit(xdata,ydata,yerr,bounds)\n",
    "                    #plot\n",
    "                    if r_squared > accept_r2:\n",
    "                        ax.plot(logcurvex,logcurvey,'r', linewidth=1)\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    #saving lot fit data into summary table\n",
    "                    #get first row index key \n",
    "                    key = df.index[0]\n",
    "                    #print(key)\n",
    "                    df.loc[key,'m'] = m\n",
    "                    df.loc[key,'b'] = b\n",
    "                    df.loc[key,'c'] = c\n",
    "                    df.loc[key,'log_r_squared'] = r_squared\n",
    "                    #print(type(popt))\n",
    "\n",
    "                    ##Fit linier function\n",
    "                    #using Scipy.optimise.curvefit then fit lin function to the data to extract the curve parameters              \n",
    "                    (m,c,r_squared,lincurvex,lincurvey) = linfit(xdata,ydata,yerr,bounds=None)\n",
    "                    \n",
    "                    #plot if R^2 is significant\n",
    "                    if r_squared > accept_r2:\n",
    "                        ax.plot(lincurvex,lincurvey,'b', linewidth=1)\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    #saving lot fit data into summary table\n",
    "                    #get first row index key \n",
    "                    df.loc[key,'slope'] = m\n",
    "                    df.loc[key,'intercept'] = c\n",
    "                    df.loc[key,'lin_r_squared'] = r_squared\n",
    "                \n",
    "                    #save plot out\n",
    "                    fig.savefig(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity_errorbarplt.png',bbox_inches='tight', dpi=300)\n",
    "                    plt.close()\n",
    "                    \n",
    "                    #saving updated summary dataframe out\n",
    "                    #print(df)\n",
    "                    df.to_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porosity in terms of fibre diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fibre constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchingdic(dat_loc,not_match,pass_lst):\n",
    "    #import required libraries\n",
    "    import os\n",
    "    \n",
    "    #create dictionary for matching parameters which do not contain the desired not matching parmeter\n",
    "    matching_dic = {}\n",
    "    #make secondary dictionary of keys as lists\n",
    "    matching_dic_key = {}\n",
    "    #itterating through each of the directories in the processed fibre directory\n",
    "    for directory in os.listdir(dat_loc):\n",
    "        #making sure to skip each of the unwanted directories in pass list\n",
    "        if directory in pass_lst:\n",
    "            pass\n",
    "        else:\n",
    "            #convert directory to string to list \n",
    "            directory = directory.split(',')\n",
    "            #print(directory)\n",
    "            #check if matching parameter list contains the desired non\n",
    "            if not_match in directory:\n",
    "                pass\n",
    "            else:\n",
    "                #convert list of matching parameters and assoicated values into separate lists of each            \n",
    "                match_values = [x for x in directory[1::2]]\n",
    "                #convert the list of matching variables into string to be used as a key in dictionary\n",
    "                match_vars = [x for x in directory[::2]]\n",
    "                match_varss = str(match_vars).strip('[]').replace(\"'\",'').replace(' ','')\n",
    "                #check if key is in matching_dic dictionary if not then add to dictionary along with associated matching parameter value\n",
    "                \n",
    "                if match_varss not in matching_dic.keys():\n",
    "                    matching_dic.setdefault(match_varss, []).append(match_values)\n",
    "                    if match_values not in matching_dic[match_varss]:\n",
    "                        matching_dic.setdefault(match_varss, []).append(match_values)\n",
    "                    else:              \n",
    "                        pass\n",
    "                \n",
    "                \n",
    "                #add list of parameters to dictionary of keys,matching_dic_key\n",
    "                if match_varss not in matching_dic_key.keys():\n",
    "                    matching_dic_key.setdefault(match_varss, []).append(match_vars)\n",
    "                    if match_values not in matching_dic[match_varss]:\n",
    "                        matching_dic_key.setdefault(match_varss, []).append(match_vars)\n",
    "                    else:              \n",
    "                        pass\n",
    "                \n",
    "    \n",
    "    return (matching_dic,matching_dic_key)\n",
    "\n",
    "\n",
    "#having created a dictionary of matching variables and associated parameter values \n",
    "#check metadata key to see if the meta data associated with the file being considered matches that of any in the dictionary for each key\n",
    "#if find match break out of loop and go to directorary of data set with same matching parameters and not matching parameters\n",
    "#open summary file and check r^2 values of each log and lin fit, extract parameters of which ever is highest \n",
    "#depending on which r^2 value is higher use equation and associated parameters to convert not matching parameter to new values\n",
    "#plot new values for not matching variable to vs uncontrolled variable\n",
    "#yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matched_para_comb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ca16e9820310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Routing through each level of the dictionary initally parasing each of matching parameter sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmatching\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatched_para_comb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#strip out whitespace from matching to find directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmatchings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matched_para_comb' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Having defined dictionary of matching parameter keys and associated parameter values for variable to be compared can then enter loop to compare them to \n",
    "#not matching parameter which is going to be converted. In this case it will be comparing micro-fibre diameter/pyridine with prosity/pyridine to get porosity/micro-fibre\n",
    "\n",
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from not_matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','') \n",
    "        \n",
    "        #before starting conversion define dictionary of matching parameter keys and associated values to compare to that of files.\n",
    "        #Do this outside for loop to minimise computations inside for loop\n",
    "\n",
    "        #using matchingdic (as defined above) create dictionary of matching parameters used to separate data which does not contain not_matching param\n",
    "        #dictionary has keys of matching parameters and their associated parameter values\n",
    "        #location of all processed fibre data\n",
    "        fibre_dat_loc = '/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/python_analysis/micro_fibre_dists/fibre_diameter/processed'\n",
    "        #check which is the desired not matching variable\n",
    "        not_match = not_matching\n",
    "        #create a list of directories to skip this prevents further manipulations applied to matching directories from breaking\n",
    "        pass_lst = ['raw_dist','hist_dist','.DS_Store']\n",
    "        #start funciton returning matching_dic as dictionary of keys of each of the matching variables and associated parameter values\n",
    "        matching_dic,matching_dic_key = matchingdic(fibre_dat_loc,not_match,pass_lst)\n",
    "        #print(matching_dic)\n",
    "\n",
    "        ##FINDING AND EXTRACTING APPROPRIATE CORRELATION AND PARAMETER VALUES\n",
    "        #Only run through this loop once to extract parameter values \n",
    "        #use second file name loop to plot converted figures\n",
    "        \n",
    "        \n",
    "        #only consider continious data so pass any datasets with catagorical uncontrolled variables\n",
    "        if 'solution_name' in not_matching or 'flat_or_fibre' in not_matching :\n",
    "            pass\n",
    "        else:\n",
    "            #create file count to minimise look ups\n",
    "            file_count = 0\n",
    "            \n",
    "            #creating holder for which equation should be used to fit to data\n",
    "            good_fit = None\n",
    "            #create holders for parameters which for specified equation \n",
    "            m = 0\n",
    "            b = 0\n",
    "            c = 0\n",
    "            #define a dictionary of pyridine concentrations and the associated micro-fibre diameter\n",
    "            mfibre_pryd = {}\n",
    "            \n",
    "            #paraseing the [matching][not_matching] files \n",
    "            for filename in matched_para_comb[matching][not_matching]:\n",
    "                \n",
    "                if file_count == 0:\n",
    "                \n",
    "                    #open distribution data as dataframe\n",
    "                    df = pd.read_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity_summary.csv', index_col = 0)\n",
    "                    #print(len(df[not_matchings].dropna()))\n",
    "\n",
    "\n",
    "                    ###Plotting contineous summary data and fit curve if there are more than 3 data points\n",
    "                    if len(df[not_matchings].dropna()) > 3:\n",
    "                        #progress the file_count\n",
    "                        file_count = file_count + 1\n",
    "\n",
    "                        #cycling through matching_dic keys, retreve list of parameters associated with each\n",
    "                        for key in matching_dic.keys():\n",
    "                            parameters = matching_dic_key[key]\n",
    "                            #reteve unique id to consider\n",
    "                            unique_id = df['unique_id'].loc[0]\n",
    "                            #print(unique_id)\n",
    "                            #look up value for element in sublist and return all values as list 'parameter_values'\n",
    "                            parameter_values = [str(sample_key.loc[sample_key['unique_id'] == unique_id, param].iloc[0]) \n",
    "                                                for sublist in parameters for param in sublist]\n",
    "                            #convert keys into list of parameters to be used to recreate file name \n",
    "                            parameters = key.split(\",\")\n",
    "                            #loop through parameters and values lists and recreate list name\n",
    "                            match_name = []\n",
    "                            for i in range(len(parameters)):\n",
    "                                match_name.append(parameters[i])\n",
    "                                match_name.append(parameter_values[i])\n",
    "                            #strip combined list to recreate list\n",
    "                            match_name = str(match_name).strip('[]').replace(' ','').replace(\"'\",'')\n",
    "                            #print(match_name)\n",
    "\n",
    "                            #having recreate the file name the go to associated summary file\n",
    "                            #setting the match file location\n",
    "                            match_fibre_dat_loc = fibre_dat_loc+'/'+match_name+'/'+not_matchings\n",
    "                            #then open the matched file location\n",
    "                            matched_summary_df = pd.read_csv(match_fibre_dat_loc+'/'+match_name+'_'+not_matchings+'_summary.csv', index_col =0)\n",
    "                            #reset index\n",
    "                            matched_summary_df = matched_summary_df.reset_index()\n",
    "\n",
    "                            #check if number of unique values in not_matching column is greater than 3 \n",
    "                            #this ensures that the right key is being used in the conversion and esnsures enough conditions have been used to assess trend\n",
    "                            uniquevalues = np.unique(matched_summary_df[not_matchings].values)\n",
    "                            if len(uniquevalues) > 3:\n",
    "                                #as long as enough points have been considered now need to check r^2 values to see if conversion is appropriate\n",
    "                                #extract log_r_squared value\n",
    "                                log_r_squared = matched_summary_df['log_r_squared'].loc[0]\n",
    "                                #print(log_r_squared)\n",
    "                                #extract lin_r_squared value\n",
    "                                lin_r_squared = matched_summary_df['lin_r_squared'].loc[0]\n",
    "                                if log_r_squared > lin_r_squared:\n",
    "                                    #check if above accept_r2\n",
    "                                    if log_r_squared > accept_r2:\n",
    "                                        #if above extract parameter values \n",
    "                                        m = matched_summary_df['m'].loc[0]\n",
    "                                        b = matched_summary_df['b'].loc[0]\n",
    "                                        c = matched_summary_df['c'].loc[0]\n",
    "                                        good_fit = 'log'\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        pass\n",
    "                                else:\n",
    "                                    if lin_r_squared > accept_r2:\n",
    "                                        #if above extract parameter values \n",
    "                                        m = matched_summary_df['slope'].loc[0]\n",
    "                                        b = matched_summary_df['intercept'].loc[0]\n",
    "                                        good_fit = 'lin'\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        break\n",
    "                    \n",
    "                    #As opening summary data only need to do this once per not_matching!\n",
    "                    \n",
    "                    ####Plotting porosity vs fibre diameter with calculated values\n",
    "                    #Having determined if there is a good fit associated with the controlled variable now want to convert\n",
    "                    #extract existing data\n",
    "                    \n",
    "                    #retreve appropriate equation for conversion\n",
    "                    if good_fit == 'log':\n",
    "                        from common_fits import LogFunc as equation\n",
    "                        parameters = (m,b,c)\n",
    "                    elif good_fit == 'lin':\n",
    "                        from common_fits import linFunc as equation\n",
    "                        parameters = (m,b)\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if good_fit != None:\n",
    "                        #convert extracted data with equation\n",
    "                        x_values = df[not_matching].dropna()\n",
    "                        x_values = np.asarray([1.0e-3 if x==0 else x for x in x_values])\n",
    "                        converted = equation(x_values,*parameters)\n",
    "                        y_values = df['median'].dropna()\n",
    "                        yerr = ((df['median']-df['25_quartile']).dropna(),(df['75_quartile']-df['median']).dropna())\n",
    "                        #xerr =\n",
    "                        #print(converted)\n",
    "\n",
    "                        #Add converted values to summary table\n",
    "                        #df = pd.concat([df, pd.DataFrame({'converted_fibre_diameter':converted})], axis=1,sort=False)\n",
    "                        #df['converted_fibre_diameter'] = converted\n",
    "                        df.loc[:,'converted_fibre_diameter'] = pd.Series(converted)\n",
    "                        #print(df.head())\n",
    "\n",
    "\n",
    "                        #plot\n",
    "                        #Initilise subplots \n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.errorbar(converted, y_values, yerr, fmt='o')\n",
    "                        ax.set(xlabel='Micro-fibre diameter ($\\mu$m)', ylabel='Porosity (%)')\n",
    "\n",
    "\n",
    "\n",
    "                        #fitting curve to points\n",
    "                        xdata = converted\n",
    "                        ydata = y_values\n",
    "                        yerr = df['IQR'].dropna()\n",
    "                        ####Fit log curve with all scalars#####\n",
    "                        #setting bounds to prevent -ve log values\n",
    "                        bounds = (0,-np.inf, -np.inf), (np.inf, np.inf, np.inf)\n",
    "                        #fit the curve\n",
    "                        (m,b,c,r_squared,logcurvex,logcurvey) = logfit(xdata,ydata,yerr,bounds)\n",
    "                        #plot\n",
    "                        #ax.plot(logcurvex,logcurvey,'r', linewidth=1)\n",
    "                        if r_squared > accept_r2:\n",
    "                            ax.plot(logcurvex,logcurvey,'r', linewidth=1)\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                        #saving fit data into summary table\n",
    "                        #get first row index key \n",
    "                        key = df.index[1]\n",
    "                        #print(key)\n",
    "                        df.loc[key,'m'] = m\n",
    "                        df.loc[key,'b'] = b\n",
    "                        df.loc[key,'c'] = c\n",
    "                        df.loc[key,'log_r_squared'] = r_squared\n",
    "                        #print(type(popt))\n",
    "\n",
    "                        ####Fit exp curve with all scalars#####\n",
    "                        #setting bounds to prevent -ve log values\n",
    "                        bounds = (0,-np.inf, -np.inf), (np.inf, np.inf, np.inf)\n",
    "                        #fit the curve\n",
    "                        (a,b,c,r_squared,expcurvex,expcurvey) = logfit(xdata,ydata,yerr,bounds)\n",
    "                        #print('exp_r_square_'+str(r_squared))\n",
    "                        #plot\n",
    "                        #ax.plot(logcurvex,logcurvey,'r', linewidth=1)\n",
    "                        if r_squared > accept_r2:\n",
    "                            ax.plot(expcurvex,expcurvey,'r', linewidth=1)\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                        ####Fit lin curve with all scalars#####\n",
    "                        #fit the curve\n",
    "                        (m,c,r_squared,lincurvex,lincurvey) = linfit(xdata,ydata,yerr,bounds=None)\n",
    "                        #plot\n",
    "                        #ax.plot(lincurvex,lincurvey,'r', linewidth=1)\n",
    "                        if r_squared > accept_r2:\n",
    "                            ax.plot(lincurvex,lincurvey,'r', linewidth=1)\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                        df.loc[key,'slope'] = m\n",
    "                        df.loc[key,'intercept'] = c\n",
    "                        df.loc[key,'lin_r_squared'] = r_squared\n",
    "\n",
    "\n",
    "                        ####Fit tanh curve with all scalars#####\n",
    "                        #fit the curve\n",
    "                        (a,b,c,d,r_squared,tanhcurvex,tanhcurvey)= tanhfit(xdata,ydata,yerr,bounds)\n",
    "                        #plot\n",
    "                        #ax.plot(tanhcurvex,tanhcurvey,'r', linewidth=1)\n",
    "                        if r_squared > 0.9:\n",
    "                            ax.plot(tanhcurvex,tanhcurvey,'r', linewidth=1)\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                        #save figure out\n",
    "                        fig.savefig(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_calculated_microfibre_porosity_errorbarplt.png',bbox_inches='tight', dpi=300)\n",
    "                        plt.close()\n",
    "\n",
    "                        #saving updated summary dataframe out\n",
    "                        #print(df)\n",
    "                        df.to_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_porosity_summary.csv')\n",
    "                    \n",
    "                    else:\n",
    "                        ####Plotting porosity vs fibre diameter with observed values\n",
    "                        #initilse figure\n",
    "                        fig, ax = plt.subplots()\n",
    "                        \n",
    "                        #print(mfibre_pryd)\n",
    "                        #itterate through the not matching values and convert them to fibre diameters with the dictionary of associated values                     \n",
    "                        x_values = [matched_summary_df.loc[matched_summary_df[not_matching] == value, 'mean'].iloc[0] for value in df[not_matching].dropna()]\n",
    "                        #print(x_values)\n",
    "                        #Setting y_values for plot as media porosities\n",
    "                        y_values = df['median'].dropna()\n",
    "                        yerr = ((df['median']-df['25_quartile']).dropna(),(df['75_quartile']-df['median']).dropna())\n",
    "                        \n",
    "                        #retieveing error in x axis\n",
    "                        #x_25_quart = [(value - matched_summary_df.loc[matched_summary_df['median'] == value, '25_quartile'].iloc[0]) for value in x_values]\n",
    "                        #print(x_25_quart)\n",
    "                        #x_75_quart = [(matched_summary_df.loc[matched_summary_df['median'] == value, '75_quartile'].iloc[0] - value) for value in x_values]\n",
    "                        #print(x_75_quart)\n",
    "                        #xerr = (x_25_quart,x_75_quart)\n",
    "                        #xerr = [matched_summary_df.loc[matched_summary_df[not_matching] == value, 'SD'].iloc[0] for value in df[not_matching].dropna()]\n",
    "                        \n",
    "                        #xerr = ((matched_summary_df['median']-matched_summary_df['25_quartile']).dropna(),(matched_summary_df['75_quartile']-matched_summary_df['median']).dropna())\n",
    "                        #xerr = matched_summary_df['median']\n",
    "                        #print(matched_summary_df.head())\n",
    "                        \n",
    "                        #plotting converted values\n",
    "                        ax.errorbar(x_values, y_values, yerr, fmt='o')\n",
    "                        ax.set(xlabel='Micro-fibre diameter ($\\mu$m)', ylabel='Porosity (%)')\n",
    "                        \n",
    "                        #Save figure out\n",
    "                        fig.savefig(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_observed_microfibre_porosity_errorbarplt.png',bbox_inches='tight', dpi=300)\n",
    "                        plt.close()\n",
    "                        \n",
    "                #This level corresponds to regardless if file count is 0 or more\n",
    "                else:\n",
    "                    pass\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
